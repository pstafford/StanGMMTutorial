---
title: "GMPE Estimation Using Stan - Tutorial"
author: "Nicolas Kuehn and Peter Stafford"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document: default
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=12,fig.height = 8, fig.path = 'pictures/')
# setwd('/Users/nico/Dropbox/WORK/STAN_Tutorial/')
```

## Introduction

This is a tutorial on how to use the program Stan (<https://mc-stan.org/>) to estimate the parameters of a ground-motion prediction equation (GMPE).
Stan is a program that uses Bayesian inference to estimate the parameters of a model via Markov Chain Monte Carlo (MCMC) sampling.

We will use an example of fitting a simple GMPE to some data.
The data come from the NGA West 2 data set.
We use a small subset 
We will fit a GMPE of the form
$$y = \theta_1 + \theta_2 M + \theta_3 M^2 + (\theta_4 + \theta_5 M) \ln \left[ R_{RUP} + h \right] + \theta_6 R_{RUP} + \theta_7 \ln \frac{V_{S30}}{760}$$
to show how one can estimate the parameters using Stan, and compare other packages such as **lme4** [@Bates2015].
The goal is not to fit a "good" GMPE that can be used in probabilistic seismic hazard analysis, but to show the principles of Stan, so that users can adjust the code to their needs.
We will gradually make the model more complex, and show the flexibility of Stan in fitting complex models, such as partially non-ergodic models, or spatial correlation models.
Only a basic familiarity with **R** is assumed.
However, it should be noted that **R** is used here as an interface to the underlying program **cmdstan**, and other interfaces exist.
Stan itself can be run through `cmdstan` as a command-line program, but can also be used through interfaces with other common dynamic programming languages such as **Julia**, **Python**, and **MATLAB**, among others (for details about these alternative interfaces, see <https://mc-stan.org/>).


## Getting Started

This tutorial uses **Stan** version 2.18.2 and requires the following **R** packages
```{r load_libraries, warning=FALSE, message=FALSE}
# load required packages
library(lme4)
library(rstan)
library(brms)
library(rstanarm)
library(bayesplot)

options(mc.cores = parallel::detectCores())
```

First, we read in the data.
We use a subset of 2000 records from the NGA West 2 data base [@Ancheta2014], from the subset used by the ASK14 model [@Abrahamson2014].
The use of 2000 records is for computational purposes.

```{r read_data}
data <- read.csv('DATA/NGA_West2_Flatfile_RotD50_d050_public_version_subsetASK.csv', header=TRUE)
dim(data)
```

The data set consists of 2000 records with 274 fields, which contain the meta data such as magnitudes, distances and so on.

First, we fit a linear model using `lmer`, from the **R** package **lme4**, to the data.
The package **lme4** is the successor to the package **nlme** and together these packages have been used quite extensively for the purposes of calibrating ground-motion models in the past.
These packages use more traditional maximum-likelihood based techniques with efficient numerical strategies to fit models. 
They are computationally very efficient, but also have limitations with regard to what type of models can be fit. 
As will be seen as we progress through this tutorial, **Stan** offers far greater flexibility and enables a broader range of models to be fit that accommodate the current needs of ground-motion model developers and researchers.

To make our GMM linear, we have to fix the parameter `h` (often referred to as pseudo-depth of near-fault-saturation-term).
In this example, we fix it to `h = 6`.


```{r fit_lme4}
h <- 6
vref <- 760
M <- data$Earthquake.Magnitude
M_sq <- M^2
R <- data$ClstD..km.
lnR <- log(R + h)
MlnR <- M * log(R + h)
lnVS <- log(data$Vs30..m.s..selected.for.analysis/vref)

EQID <- data$EQID
STATID <- data$Station.Sequence.Number

Y <- log(data$PGA..g.)

data_regression <- data.frame(M,M_sq,R,lnR,MlnR,lnVS,Y,EQID)

fit_lmer <- lmer(Y ~ 1 + M_sq + lnR + M * lnR + lnVS
                + (1|EQID), data=data_regression)
```

This code fits a linear mixed effects model with one random effect for earthquakes (commonly called event terms).
This type of model has been referred to as a 'random effects' model within the engineering seismology community [@Abrahamson1992].
We can look at the output with:

```{r output_lme4}
summary(fit_lmer)
```

### Stan

Now we describe how to fit the same model using Stan.
A Stan program is made up of blocks, like a `data {}`, `parameters {}` and a `model {}` block.
These are used to declare the data, the parameters to be estimated, and a generative model for the data.
A declaration of a variable will look like `real a;` to declare a variable `a` that is a real, or `vector[N] Y;` to declare a vector of length `N`.
Stan is typed, so there is a difference between a declaration `real a;` or `int a;`.
Constraints can be declared as `real<lower=L,upper=U> a;`, which means that `a` can take only values `L <= a <= U`.
Each line in a stan program has to end in `;`.

In the Stan program below, we first declare the number of records `N` and the number of events `NEQ` as integer values.
We then declare the target and predictor variables as vectors of length `N`.
Alternatively, they could also be declared as an array, via `real M[N]`.
We also declare an integer array `int idx_eq[N]` which stores the event indices as numbers between `1` and `NEQ`.

Next we have a `transformed data {}` block.
This block is optional, and can be used to define global variables that are used throughout the program.

In the `parameters {}` block the coefficients `theta`, the standard deviations `phi` and `tau` and the event terms `deltaB` are declared.
Since standard deviations need to be positive, they are declared as `real<lower=0> tau` and `real<lower=0> phi`.

The `model {}` block contains the generative model, which includes the functional form for our GMPE, but also defines the prior distributions for the parameters to be estimated.
There is a loop over all records, and inside the loop we caclulate the median prediction for each record (including the event term).
The data is assumed to be normally-distributed with mean equal to the event term corrected median prediction and standard deviation `phi`.
The loop shows the declaration of a local variable `mu`, which is local to the `for` loop.
Since Stan estimates parameters via Bayesian inference, we need to specify prior distributions for the parameters, which are the `theta1 ~ normal(0,10)` statements - in this case, the prior distribution for `theta1` is a normal distribution with mean zero and standard deviation 10.
If no prior distributions are specified, Stan will assume an improper uniform prior over the values for which the parameter is declared.
Specification of prior distributions is an important and often-discussed topic - we recommend checking out the prior recommendation wiki for some guidelines (<https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations>).
The standard deviations are given half-cauchy distributions (since they are constrained to be positive), which is the default recommendation in Stan.
In our experience, these work well for $phi$, $tau$ and $phi_{SS}$ and $phi_{S2S}$, but might have too heavy tails for some partially nonergodic parameters.


```{stan, output.var="gmm_model1"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
}

transformed data {
  real h = 6;
  real vref = 760;
}

parameters {
  real theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real theta6;
  real theta7;
  
  real<lower=0> phi;
  real<lower=0> tau;
  
  vector[NEQ] deltaB;
}

model {
  theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  theta6 ~ normal(0,10);
  theta7 ~ normal(0,10);
  
  phi ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaB ~ normal(0,tau);
  
  for(i in 1:N) {
    real mu;
    mu = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + h) + theta6 * R[i] + theta7 * log(VS[i]/vref) + deltaB[idx_eq[i]];
    Y[i] ~ normal(mu,phi);
  }
}
```

A more efficient version of the above Stan code is provided below.
This version is vectorized and uses matrix algebra, which makes it run faster.
Note that this efficiency is not related to the use of vectorization _per se_, as the Stan program is compiled to **C++** before being called from Stan.
The increase in efficiency is more related to how the derivatives of the likelihood function are generated.
The NUTS sampler used by Stan requires derivatives of the likelihood function to be computed and efficiency gains in Stan are usually related to how to optimally represent the derivative tree required as part of this process.

```{stan, output.var="gmm_model1_vectorized"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
}

transformed data {
  real h = 6;
  real vref = 760;
  
  matrix[N,7] X;
  
  for(i in 1:N) {
    X[i,1] = 1;
    X[i,2] = M[i];
    X[i,3] = square(M[i]);
    X[i,4] = log(R[i] + h);
    X[i,5] = M[i] * log(R[i] + h);
    X[i,6] = R[i];
    X[i,7] = log(VS[i]/vref);
  }
}

parameters {
  vector[7] theta;
  
  real<lower=0> phi;
  real<lower=0> tau;
  
  vector[NEQ] deltaB;
}

model {
  theta ~ normal(0,10);
  
  phi ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaB ~ normal(0,tau);
  
  Y ~ normal(X * theta + deltaB[idx_eq],phi);
}
```

There are two ways to run the code. One is to put the Stan code into a variable within **R**:
`
model1 <- '
data {...}
parameters {...}
model {...}'
`
and then run the code with `rstan(model1)`. The other one is to save the code in a file with the extension `.stan` and then run it with `rstan('path-to-file/file.stan')`.
Below, we coded up the vectorized model, which resides in a file called `gmm_model1_vectorized.stan` in subdirectory `STAN`.

``` {r run_stan_gmm_model1}
eq_idx_factor <- factor(data$EQID)
eq_idx <- as.numeric(eq_idx_factor)

full_d <- list(
  N = length(data[,1]),
  NEQ = max(eq_idx),
  idx_eq = eq_idx,
  M = data$Earthquake.Magnitude,
  R = data$ClstD..km.,
  VS = data$Vs30..m.s..selected.for.analysis,
  Y =log(data$PGA..g.)
);

niter = 400;  # total number of MCMC iterations (warmup + actual sampling)
wp = 200;     # number of warm-up iterations (performed prior to actual sampling)
nchains = 4;  # number of MCMC chains, run in parallel, each from a different random seed

fit_model1 <- stan('STAN/gmm_model1_vectorized.stan', data = full_d, 
                       iter = niter, chains = nchains, warmup = wp, verbose = FALSE)
```

Generally, it is a good idea to check whether the maximum treedepth is exceeded and whether there are divergences to see if there were problems with the fit.

```{r check_fit_model1}
check_treedepth(fit_model1)
check_divergences(fit_model1)
```

One drawback of using Stan is the runtime, which is considerably longer than for `lmer`.
This is not so much a problem for the small subset of data used here, but can become a problem for a regression with thousands of records for multiple periods, especially if the model is more complex.
However, once a Bayesian model is regressed and used for Bayesian updating, this becomes less of an issue [@Stafford2018].

``` {r runtime_model1}
print(get_elapsed_time(fit_model1))
```

We can look at a summary using the `print` function, and look at trace plots of some parameters via `traceplot`.
The option `pars = ` determines which parameters we want to look at (there are `r max(eq_idx)` event terms $\delta B$, so we only look at the fixed effects and standard deviations).
The output of `print` shows the mean, standard deviation and and some quantiles, derived from the posterior distribution (the posterior samples), as well as summary statistics like `Rhat`, which gives an indication of whether the chains have converged.
An alternative to `print` is the `summary` function, which gives summaries for the individual chains, as well as the combined posterior draws.
The trace plots show the mixing of the different chains.

```{r summary_model1}
print(fit_model1, pars = c('lp__','theta','phi','tau'))
traceplot(fit_model1,pars = c('lp__','theta','phi','tau'))
```

We can also plot uncertainty intervals of the posterior distribution with
``` {r plot_model1}
plot(fit_model1,pars = c('phi','tau'))
```

Comparing the values from Stan and from **lme4**, they are similar.
The posterior samples can be assessed with `extract(fit_model1, pars = c('deltaB'))`, which returns a list, where every element is an array with the samples for the parameter.
For example, with the code below we plot the means of the posterior distribution of the event terms against magnitude, as well as a histogram of the posterior samples for the magnitude scaling parameter.

``` {r extract_posterior_model1}
posterior <- extract(fit_model1)
deltaB_mean <- colMeans(posterior$deltaB)
M_eq <- unique(data.frame(eq_idx,data$Earthquake.Magnitude[eq_idx]))[,2]
par(mfrow = c(1,2))
plot(M_eq,deltaB_mean)
hist(posterior$theta[,2])
```

For a full list of the methods that are available to look at output from a Stan model, see `help("stanfit")`.

# Bayesplot

The **bayesplot** package has implemented some functions for diagnosing and assessing posterior distributions of a fit with Stan.

# BRMS

The package **brms** [@Burkner2017] is a wrapper for Stan that allows one to use a syntax similar to `lmer`.
It is very flexible in the models that it can fit.

``` {r fit_brm}
fit_brm <- brm(Y ~ 1 + M_sq + lnR + M * lnR + lnVS + (1|EQID),data = data_regression)
summary(fit_brm)
```

## More Complicated Models

An obvious extension to the simple model is to include a random systematic effect for stations, and to make the near-fault-saturation term, $h$, a parameter of the model.
Estimating $h$ makes the model nonlinear, so we cannot use the fully vectorized model anymore.
The Stan code is below.
We added the number of stations and a station index to the data, and moved `h` from the transformed data block to the parameters block.
Since `h` should be positive, we declare it with `<lower=0>`.
The median is assigned in a loop because the functional form is nonlinear, but the sampling statement `Y ~ normal(mu,sigma)` is vectorized, which requires that `mu` is declared as a vector.

```{stan, output.var="gmm_model2"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
}

transformed data {
  real vref = 760;
}

parameters {
  real theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real theta6;
  real theta7;
  
  real<lower=0> h;
  
  real<lower=0> phiSS;
  real<lower=0> tau;
  real<lower=0> phiS2S;
  
  vector[NEQ] deltaB;
  vector[NSTAT] deltaS;
}

model {
  vector[N] mu;

  theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  theta6 ~ normal(0,10);
  theta7 ~ normal(0,10);
  h ~ normal(6,4);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaB ~ normal(0,tau);
  deltaS ~ normal(0,phiS2S);
  
  for(i in 1:N) {
    mu[i] = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + h) + theta6 * R[i] + theta7 * log(VS[i]/vref) + deltaB[idx_eq[i]] + deltaS[idx_stat[i]];
  }
  Y ~ normal(mu,phiSS);
}
```

Often, `h` is modeled as being magnitude dependent, such as $h = a \exp(bM)$.
This can be easily incorporated.

```{}

parameters {
...
  real a;
  real b;
}

model {}
...
  for(i in 1:N) {
    mu[i] = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + a * exp(b * M[i])) + theta6 * R[i] + theta7 * log(VS[i]/vref) + deltaB[idx_eq[i]] + deltaS[idx_stat[i]];
  }
  Y ~ normal(mu,phiSS);
}
```

Since `a` and `b` are typically hard to estimate from data, they should be given informative priors.

# Correlated Random Effects

Below, we show an example of how one could model multiple random effects for the same level.
We assume that for each event, there is an event-specific near-fault-saturation term, which is distributed around a mean function $a \exp (b M)$, i.e.
$$
h_e = a \exp(b M_e + \delta B_{2,e})
$$
First, we just add a new variable for this term to the model.
```{stan, output.var="gmm_model2_correlated_deltaB"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
}

transformed data {
  real vref = 760;
}

parameters {
  real theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real theta6;
  real theta7;
  
  real a;
  real b;
  
  real<lower=0> phiSS;
  real<lower=0> tau;
  real<lower=0> phiS2S;
  real<lower=0> tau_h;
  

  vector[NEQ] deltaB;
  vector[NEQ] deltaB2;
  
  vector[NSTAT] deltaS;
}

model {
  vector[N] mu;

  theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  theta6 ~ normal(0,10);
  theta7 ~ normal(0,10);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaS ~ normal(0,phiS2S);
  deltaB ~ normal(0,tau);
  deltaB2 ~ normal(0,tau_h);
  
  for(i in 1:N) {
    mu[i] = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + a * exp(b * M[i] + deltaB2[idx_eq[i]])) + theta6 * R[i] + theta7 * log(VS[i]/vref) + deltaB[idx_eq[i]] + deltaS[idx_stat[i]];
  }
  Y ~ normal(mu,phiSS);
}
```

Next, we model the two event terms as correlated, _i.e._, distributed according to a multivariate normal distribution
$$
\vec{\delta B} \sim N(\vec{0},\boldsymbol{\Sigma})
$$
Thus, we now declare the event terms `deltaB` as an array of length `NEQ` of two-dimensional vectors, corresponding to the constant random effect and the event-specific near-fault-term.
The prior for the covariance matrix $\Sigma$ is separated into a prior for the standard deviations `tau` and one for the correlation marix `C_eq`, which is based on [@Lewandowski2009].
These are combined into the covariance matrix via `Sigma_eq = quad_form_diag(C_eq,tau)`, where `quad_form_diag(C_eq,tau) = diag_matrix(tau) * C_eq * diag_matrix(tau)`.

```{stan, output.var="gmm_model2_correlated_deltaB"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
}

transformed data {
  real vref = 760;
  vector[2] mu_eq;
  
  for(i in 1:2)
    mu_eq[i] = 0;
}

parameters {
  real theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real theta6;
  real theta7;
  
  real a;
  real b;
  
  real<lower=0> phiSS;
  vector<lower=0>[2] tau;
  real<lower=0> phiS2S;
  
  corr_matrix[2] C_eq;
  vector[2] deltaB[NEQ];
  
  vector[NSTAT] deltaS;
}

model {
  vector[N] mu;
  matrix[2,2] Sigma_eq;

  theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  theta6 ~ normal(0,10);
  theta7 ~ normal(0,10);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaS ~ normal(0,phiS2S);
  
  C_eq ~ lkj_corr(2);
  Sigma_eq = quad_form_diag(C_eq,tau);
  deltaB ~ multi_normal(mu_eq,Sigma_eq);
  
  for(i in 1:N) {
    mu[i] = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + a * exp(b * M[i] + deltaB[idx_eq[i],2])) + theta6 * R[i] + theta7 * log(VS[i]/vref) + deltaB[idx_eq[i],1] + deltaS[idx_stat[i]];
  }
  Y ~ normal(mu,phiSS);
}
```

This model can also be coded in a different way, based on the Cholesky factorization of the correlated event terms, which should be more efficient.

```{stan, output.var="gmm_model2_correlated_deltaB2"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
}

transformed data {
  real vref = 760;
  vector[2] mu_eq;
  
  for(i in 1:2)
    mu_eq[i] = 0;
}

parameters {
  real theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real theta6;
  real theta7;
  
  real a;
  real b;
  
  real<lower=0> phiSS;
  vector<lower=0>[2] tau;
  real<lower=0> phiS2S;
  
  cholesky_factor_corr[2] L_eq;
  matrix[2,NEQ] z_eq;
  
  vector[NSTAT] deltaS;
}

transformed parameters {
  matrix[2,NEQ] deltaB;

  deltaB = diag_pre_multiply(tau, L_eq) * z_eq;

}

model {
  vector[N] mu;
  matrix[2,2] Sigma_eq;

  theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  theta6 ~ normal(0,10);
  theta7 ~ normal(0,10);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaS ~ normal(0,phiS2S);
  
  L_eq ~ lkj_corr_cholesky(2);
  to_vector(z_eq) ~ normal(0,1);
  
  
  for(i in 1:N) {
    mu[i] = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + a * exp(b * M[i] + deltaB[2,idx_eq[i]])) + theta6 * R[i] + theta7 * log(VS[i]/vref) + deltaB[1,idx_eq[i]] + deltaS[idx_stat[i]];
  }
  Y ~ normal(mu,phiSS);
}
```


## Partially Nonergodic Models

Over the last few years it has been recognized that the ergodic assumption [@Anderson1999] (that the ground-motion distribution at a site over time is the same as the ground-motion distribution over space) can lead to biased hazard results.
With an increasing amount of data in different regions, the ergodic assumption can be relaxed.
An intermediate step towards fully nonergodic models are partially nonergodic models (though one can argue that models that account for systematic station terms $\delta S$ are already partially nonergodic) in which some of the parameters are different for different regions.
Often, the constant, anelastic attenuation coefficient, and the site-scaling coefficient are regionally dependent [@Stafford2014,@Kotha2016,@Sedaghati2017,@Kuehn2016].
It makes sense to model these as regional random effects, since in that case the coefficient for regions with a smaller amount of data are automatically associated with larger uncertainty (in the Bayesian case, a wider posterior distribution).
Typically, one assumes that the regional random effects are distributed according to a normal distribution, where the regional coefficients are samples from a global coefficient (_e.g._, for the constant $\theta_1$)
$$
\theta_1 \sim \mathcal N(\mu_{\theta 1},\sigma_{\theta 1})
$$
where $\mu_{\theta_1}$ is the (global) mean (over the data set) for the constant coefficient and $\sigma_{\theta_1}$ is the standard deviation which determines how much the regional coefficients can differ from the global mean.
Below, we have written a Stan model with regional coefficients for the constant, the anelastic attenuation (linear R scaling) and the $V_{S30}$-scaling.
We add an integer for the number of regions, as well as an integer comprising the region indices for the records to the `data {}` block.
We then declare the mean coefficients and vectors for the regional coefficients, as well as the standard deviations.
In this case, the regional parameters are declared as independent, but they can also be modeled as correlated as explained previously.
We have declared the regional parameters `theta6` (linear R term) with `vector<upper=0>[NREG] theta6`, so they are constrained to be negative.
This is a physical requirement, but in particular for partially nonergodic models it can happen that a regional coefficient becomes positive if data is sparse - this can also happen for long periods, where the coefficient typically approaches zero.
Imposing the constraint ensures that this does not happen.


```{stan, output.var="gmm_model_partially_nonergodic"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  int<lower=1> NREG; // number of regions
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
  int<lower=1,upper=NREG> idx_reg[N];
}

transformed data {
  real vref = 760;
}

parameters {
  real mu_theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real mu_theta6;
  real mu_theta7;
  
  real<lower=0> h;
  
  real<lower=0> phiSS;
  real<lower=0> tau;
  real<lower=0> phiS2S;
  
  real<lower=0> sigma_theta1;
  real<lower=0> sigma_theta6;
  real<lower=0> sigma_theta7;
  
  vector[NEQ] deltaB;
  vector[NSTAT] deltaS;
  
  vector[NREG] theta1;
  vector<upper=0>[NREG] theta6;
  vector[NREG] theta7;
}

model {
  vector[N] mu;

  mu_theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  mu_theta6 ~ normal(0,10);
  mu_theta7 ~ normal(0,10);
  h ~ normal(6,4);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  sigma_theta1 ~ cauchy(0,0.5);
  sigma_theta6 ~ cauchy(0,0.01);
  sigma_theta7 ~ cauchy(0,0.3);
  
  deltaB ~ normal(0,tau);
  deltaS ~ normal(0,phiS2S);
  
  theta1 ~ normal(mu_theta1,sigma_theta1);
  theta6 ~ normal(mu_theta6,sigma_theta6);
  theta7 ~ normal(mu_theta7,sigma_theta7);
  
  for(i in 1:N) {
    mu[i] = theta1[idx_reg[i]] + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + h) + theta6[idx_reg[i]] * R[i] + theta7[idx_reg[i]] * log(VS[i]/vref) + deltaB[idx_eq[i]] + deltaS[idx_stat[i]];
  }
  Y ~ normal(mu,phiSS);
}
```

The model can be rewritten, using a so-called non-centered parameterization, by recognizing that
$$
\theta_1 \sim \mathcal N(\mu_{\theta 1},\sigma_{\theta 1})
$$
is the same as
$$
\theta_1 = \mu_{\theta 1} + z \sigma_{\theta 1}\\
z \sim \mathcal N(0,1)
$$
Hence, we now declare a vector `z` of length `NREG` for each regionally varying coefficient, which has a standard normal prior distribution, and calculate the parameters in the `transformed parameters {}` block according to the above equation.
To ensure that `theta6` is positive, we updated the upper limit for `z6` (this also serves as an example that parameters can be used as upper/lower limits).
The rest of the model is the same.

```{stan, output.var="gmm_model_partially_nonergodic_noncentered"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  int<lower=1> NREG; // number of regions
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
  int<lower=1,upper=NREG> idx_reg[N];
}

transformed data {
  real vref = 760;
}

parameters {
  real mu_theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real mu_theta6;
  real mu_theta7;
  
  real<lower=0> h;
  
  real<lower=0> phiSS;
  real<lower=0> tau;
  real<lower=0> phiS2S;
  
  real<lower=0> sigma_theta1;
  real<lower=0> sigma_theta6;
  real<lower=0> sigma_theta7;
  
  vector[NEQ] deltaB;
  vector[NSTAT] deltaS;
  
  vector[NREG] z1;
  vector<upper=-mu_theta6/sigma_theta6>[NREG] z6;
  vector[NREG] z7;
}

transformed parameters {
  vector[NREG] theta1;
  vector[NREG] theta6;
  vector[NREG] theta7;
  
  theta1 = mu_theta1 + z1 * sigma_theta1;
  theta6 = mu_theta6 + z6 * sigma_theta6;
  theta7 = mu_theta7 + z7 * sigma_theta7;
}

model {
  vector[N] mu;

  mu_theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  mu_theta6 ~ normal(0,10);
  mu_theta7 ~ normal(0,10);
  h ~ normal(6,4);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  sigma_theta1 ~ cauchy(0,0.5);
  sigma_theta6 ~ cauchy(0,0.01);
  sigma_theta7 ~ cauchy(0,0.3);
  
  deltaB ~ normal(0,tau);
  deltaS ~ normal(0,phiS2S);
  
  z1 ~ normal(0,1);
  z6 ~ normal(0,1);
  z7 ~ normal(0,1);
  
  for(i in 1:N) {
    mu[i] = theta1[idx_reg[i]] + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + h) + theta6[idx_reg[i]] * R[i] + theta7[idx_reg[i]] * log(VS[i]/vref) + deltaB[idx_eq[i]] + deltaS[idx_stat[i]];
  }
  Y ~ normal(mu,phiSS);
}
```


In the previous two partially nonergodic models, the prior distribution for the regional standard deviations `sigma_theta1`, `sigma_theta6` and `sigma_theta7` was a half-cauchy distribution.
This is based on a recommendation by [@Gelman2006b].
However, the half-cauchy distribution has very heavy tails, which can lead to unrealistically high values of the standard deviation and thus spurious regional deviations from the global mean coefficient (the deviation is $z \sigma_{\theta}$) if the number of regions is small.
This is also a problem of maximum-likelihood estimation with a small number of groups.
For event terms and station terms, this is less of a problem since there are many events/stations for each data set, but the number of regions is typically small (<10), which can make it hard to estimate the regional standard deviation.
In that case, stronger prior information is needed - for example, an exponential distribution or a normal or Student-t distribution might be better.
These can be implemented as

``` {}
  sigma_par1 ~ exponential(1);
  sigma_par1 ~ normal(0,1);
  sigma_par1 ~ student_t(6,0,1);
```

What is generally important is that the prior distribution should be scaled based on the effects.
This is a complicated topic, and general advice that works in every situation is difficult.

## Robust Regression

Sometimes a data set has outliers, and these outliers can severley affect both the mean and standard deviation ($\phi$) of an estimated model - often, GMPE developers discard some records which are obvious outliers (these might be of low quality due to processing errors).
One way to mitigate the effect of outlier data points on the model is to use robust regression - in robust regression, one tries to limit the influence of a data point that is far from the regression line.
For example, one way to do that would be to minimize the absolute residuals and not the squared residuals.
In a Bayesian model, one way to do robust regression is to replace the data likelihood (which until now has been based upon the assumption that logarithmic ground-motions are normally distributed, when conditioned upon some rupture scenario) with a Student-_t_ distribution with low degrees-of-freedom.
Such a distribution has heavier tails than the normal distribution and thus is less sensitive to outliers.
Such a model can be coded in Stan as follows (see also <http://doingbayesiandataanalysis.blogspot.com/2013/06/bayesian-robust-regression-for-anscombe.html> or <https://jrnold.github.io/bayesian_notes/robust-regression.html>)

``` {}
...
parameters {
  real<lower=1> nu;
}
model {
  ...
  nu ~ gamma(2,0.1);
  ...

  Y ~ student_t(nu,mu,sigma);
}
...
```

Here, we declare a new parameter `nu` for the degrees-of-freedom, which is given a gamma prior (following [@Juarez2010]).
The rest of the model is the same, except that the sampling statement for the data is changed from the normal distribution to the Student-_t_ distribution.

## Heteroscedastic models

``` {}
...
parameters {
  ...
  real<lower=0> tau1;
  real<lower=0> tau2;

}

transformed parameters {
  vector[NEQ] tau;
  
  for(i in 1:NEQ) {
    if(M[i] < 5)
      tau[i] = tau1;
    else if (M[i] < 6)
      tau[i] = tau1 + (tau2 - tau1) * (M[i] - 5);
    else
      tau[i] = tau2;
  }
}

model {
  ...
  deltaB ~ normal(0,tau);
}
```

## Truncated regression

If data comes from _e.g._, triggered instruments, it is truncated.
If this is not taken into account, the regression estimates will be biased [@Chao2018,@Bragato2004].
Truncated regression can be performed in Stan using the following adjustment to the sampling statement.
Truncated sampling statements cannot be vectorized, so it is moved into the loop (which unfortunately is less efficient).


``` {}
data {
  ...
  real trigger_level;
}
...

model {
  ...
  for(i in 1:N) {
    real mu;
    mu = ...;
    Y[i] ~ normal(mu,phiSS) T(trigger_level,);
  }
}
```

## Missing Data

Sometimes, there is missing data for some records.
Typically in GMPE estimation, these records are ignored.
However, there are methods how one can deal with missing data [@Rubin1976a,@Allison2002].
A standard method is multiple imputation, for example implemented in the R-package `mi` [@Su2011] or `mice` [@vanBuuren2011].
In Stan (or Bayesian inference in general), one can declare a missing data point as a parameter in the model, which is estimated.
Since a posterior distribution is estimated for the missing data (with different samples), this functions as some sort of multiple imputation.

Below, we show an example, where we assume that some $V_{S30}$-values are missing.
Since each station has a unique $V_{S30}$-value, that means that we declare the input $V_{S30}$-values for each stations via `vector[N] VS;`.
Similar to the station term,this means that we also need the index connecting stations to records.
We also need a new index which comprises the indices of the missing values, as well as the number of stations with missing values.
For example, if the second and tenth station had an unknown $V_{S30}$-value, then the array of missing values would be of length 2 and consists of `c(2,10)`.

The missing values are declared as `vector[N_missing_VS] VS_missing;`.
Since we cannot assign new values to loaded data, we declare a new variable in the `transformed parameters {}` block, which contains the original `VS` values, and the estimated missing values for the correct indices.
This new variable is then used in the calcuation of the median prediction.

The missing values should be assigned an informative prior distribution.
If this is not available, one could use the (geometric) mean and standard deviation of the available $V_{S30}$-values.
Instead of estimating $V_{S30}$, one could also estimate $\ln V_{S30}$.

``` {stan, output.var = "gmm_model2_missingVS"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  int<lower=1,upper=NSTAT> N_missing_VS;
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[NSTAT] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
  int<lower=1,upper=NSTAT> idx_missing_vs[N_missing_VS];
}

transformed data {
  real vref = 760;
}

parameters {
  real theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real theta6;
  real theta7;
  
  real<lower=0> h;
  
  real<lower=0> phiSS;
  real<lower=0> tau;
  real<lower=0> phiS2S;
  
  vector[NEQ] deltaB;
  vector[NSTAT] deltaS;
  
  vector<lower=0>[N_missing_VS] VS_missing;
}

transformed parameters {
  vector[NSTAT] VS_imputed;
  
  VS_imputed = VS;
  
  for(i in 1:N_missing_VS)
    VS_imputed[idx_missing_vs[i]] = VS_missing[i];
}

model {
  vector[N] mu;

  theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  theta6 ~ normal(0,10);
  theta7 ~ normal(0,10);
  h ~ normal(6,4);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaB ~ normal(0,tau);
  deltaS ~ normal(0,phiS2S);
  
  VS_missing ~ lognormal(log(400),0.5); // some prior should be set (if no information, maybe mean/sd of dataset)
  
  for(i in 1:N) {
    mu[i] = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + h) + theta6 * R[i] + theta7 * log(VS_imputed[i]/vref) + deltaB[idx_eq[i]] + deltaS[idx_stat[i]];
  }
  Y ~ normal(mu,phiSS);
}
```

## Measurement error models

See [@Kuehn2018] (need to expand).

## Incorporating Spatial Correlation

Ground-motion observations made at closely-spaced locations will be correlated by virtue of the fact that they sample similar travel paths from source to site.
This means that directionality effects from the source will be similar and that near-surface crustal amplification effects will also be similar.
The models considered thus far do not account for this.
Studies like @Jayaram:2009gm derive spatial correlation models of the form:
$$
  \rho(\boldsymbol{x}_i,\boldsymbol{x}_j) = \exp\left( -\frac{||\boldsymbol{x}_i-\boldsymbol{x}_j||}{r_c}\right)
$$
using within-event residuals computed from ground-motion models.
Within-event residuals $\boldsymbol{\varepsilon}(\boldsymbol{x})$ are computed for all observations made for each event and the spatial correlation among these observations is well represented by the exponential model shown above that depends upon a single parameter $r_c$ which is the correlation length.
@Jayaram:2010bo showed how to account for some known degree of spatial correlation within the traditional random-effects regression framework of [@Abrahamson1992] when only a random effect for each event is considered.
The extension to more elaborate cases where multiple random effects are considered is more challenging to deal with, but can be handled using `stan` [@Stafford:2018dn].

Note that @Jayaram:2009gm work with within-event residuals, but not site-corrected within-event residuals. 
As a result, they find that the correlation length $r_c$ differs from region-to-region depending upon the degree of correlation among sites with similar levels of $V_{S,30}$.
In a model where random effects are considered for each site we should expect less spatial correlation as the site random effects will be spatially correlated and this portion of the correlation is no longer contained in the site-corrected within-event residuals.
In `stan`, random effects for site, or for event, can also be regarded as being spatially-correlated, but in the example below we focus upon the representation of the site-corrected within-event residuals.

In the previously considered models the covariance matrix of the within-event residuals was equivalent to:
$$
  \boldsymbol{\Sigma_\varepsilon} = \phi_{SS} \boldsymbol{I}_{n}
$$
where $\boldsymbol{I}_n$ is an $n\times n$ identity matrix.
When spatial correlations are considered this changes to:
$$
  \boldsymbol{\Sigma_\varepsilon} = \phi_{SS} \boldsymbol{\Lambda}_{n}(\boldsymbol{x};r_c)
$$
where $\boldsymbol{\Lambda}_{n}(\boldsymbol{x};r_c)$ is an $n\times n$ correlation matrix that is obtained by computing the exponential correlation as a function of inter-station distances and the correlation length $r_c$.
However, as we only consider the spatial correlations within each event we have a block-diagonal structure for $\boldsymbol{\Lambda}_{n}(\boldsymbol{x};r_c)$.
$$
  \boldsymbol{\Lambda}_{n}(\boldsymbol{x};r_c) \equiv \boldsymbol{\Lambda}_{n_{rec}^{(1)}}(\boldsymbol{x};r_c,eq^{(1)}) \oplus \boldsymbol{\Lambda}_{n_{rec}^{(2)}}(\boldsymbol{x};r_c,eq^{(2)}) \oplus \ldots \oplus \boldsymbol{\Lambda}_{n_{rec}^{(n_{eq})}}(\boldsymbol{x};r_c,eq^{(n_{eq})})
$$
where $\oplus$ is the direct sum operator, and the $\boldsymbol{\Lambda}_{n_{rec}^{(i)}}$ represents the within-event covariance matrix for earthquake $i$. 
This within-event covariance matrix is of size $n_{rec}^{(i)}\times n_{rec}^{(i)}$, with $\sum_{i}^{n_{eq}} n_{rec}^{(i)} = n$

Note that this block diagonal structure holds only for the event- and site-corrected within-event residuals.
The inclusion of random effects for events also has a block diagonal structure of the same form as the spatial covariance matrix $\boldsymbol{\Lambda}(\boldsymbol{x};r_c)$, but considering random effects for sites adds elements that _link_ blocks by virtue of the fact that the same sites record multiple events.

A current limitation of `stan` is that it does not have the ability to handle _ragged_ data structures.
That is, we cannot define an array that hold arrays or matrices of differing size as its elements.
When working with correlations among the observations of each event such a data structure would be very useful as it would allow us to define inter-station distance matrices for each event seperately.
Instead we can have a few options:  
* manually define $n_{eq}$ distance matrices for each event (either passed to `stan` as data, or created within `stan` from station coordinates);  
* work with a single very large $n\times n$ distance, and hence correlation and covariance matrix; or,  
* have some large $n\times n$ distance matrix, but only work with event-specific blocks for the purposes of sampling  

The first of these options, while ultimately the most computationally efficient, does not scale well to analyses that consider a large number of events. 
The second option, while the easiest to implement programatically, is the least computationally efficient.
Generally speaking, matrix operations (like Cholesky factorisation, matrix inversion, _etc_) tend to scale as $\mathcal{O}(n^3)$.
Therefore, the third option of accessing sub-blocks tends to be the preferred option currently.
Accessing sub-blocks of the matrix requires a local copy to be made which has a cost associated with it, but this is offset by enabling the use of smaller $n$ for each matrix operation.
The example code below shows the use of this latter approach.

### Known spatial correlation length

This code corresponds to a case in which the correlation length is assumed known ahead of the regression analysis and so the spatial correlation matrix can be passed with its block-diagonal structure as data.
Also passed is `ObsPerEvent` which is an array of length `NEQ` defining how many records each event has.
This array is used to define indexing into the vectors of observations and means as well as the covariance matrix using calls such as:
```{}
...
vector[ObsPerEvent[i]] eventMu = segment(mu, epos, ObsPerEvent[i]);
matrix[ObsPerEvent[i],ObsPerEvent[i]] eventLCov = phiSS * block( LspatialCorrelationMatrix, epos, epos, ObsPerEvent[i], ObsPerEvent[i] );
...
```
where `segment(x, i, n)` extracts a portion of an array `x` starting at index `i` and inclusively taking the `n` consecutive elements.
Similarly, `block(X, i, j, ni, nj)` extracts a sub-matrix from `X` starting from index `(i,j)` and taking `ni` elements in the first dimension and `nj` elements in the second dimension.
In order for this approach to work, the observations need to be sorted so that they are grouped by event and that this indexing operations will be accessing contiguous data corresponding to each event.

```{stan, output.var = "gmm_model_spatial_correlation_fixed"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
  
  matrix[N,N] spatialCorrelationMatrix; // within-event correlation matrix (block diagonal matrix)
  int<lower=1,upper=NEQ> ObsPerEvent[NEQ];
}

transformed data {
  real vref = 760;
  // taking Cholesky here is only valid if spatial correlation matrix is block diagonal
  cholesky_factor_corr[N] LspatialCorrelationMatrix = cholesky_decompose(spatialCorrelationMatrix);
}

parameters {
  real theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real theta6;
  real theta7;
  
  real<lower=0> h;
  
  real<lower=0> phiSS;
  real<lower=0> tau;
  real<lower=0> phiS2S;
  
  vector[NEQ] deltaB;
  vector[NSTAT] deltaS;
}

model {
  vector[N] mu;

  theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  theta6 ~ normal(0,10);
  theta7 ~ normal(0,10);
  h ~ normal(6,4);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaB ~ normal(0,tau);
  deltaS ~ normal(0,phiS2S);
  
  for(i in 1:N) {
    mu[i] = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + h) + theta6 * R[i] + theta7 * log(VS[i]/vref) + deltaB[idx_eq[i]] + deltaS[idx_stat[i]];
  }

  {
    int epos;
    epos = 1;
    for ( i in 1:NEQ ) {
      if ( ObsPerEvent[i] > 1 ) {
        vector[ObsPerEvent[i]] eventMu = segment(mu, epos, ObsPerEvent[i]);
        matrix[ObsPerEvent[i],ObsPerEvent[i]] eventLCov = phiSS * block( LspatialCorrelationMatrix, epos, epos, ObsPerEvent[i], ObsPerEvent[i] );
        segment(Y, epos, ObsPerEvent[i]) ~ multi_normal_cholesky( eventMu, eventLCov );
      } else {
        real eventMu = mu[epos];
        Y[epos] ~ normal( eventMu, phiSS );
      }
      epos = epos + ObsPerEvent[i];
    }
  }
}
```

### Unknown spatial correlation length
In the case above we already knew the spatial correlation and so could compute the cholesky factor of the spatial correlation matrix within the `transformed data` block.
In the case where we wish to solve for the spatial correlation length, things are not so simple.
Now, as the correlation length will vary as a parameter of the model, the correlation matrix for each event will also be varying with every sample.
The only attribute that remains fixed is the inter-station distance matrix.
The code below demonstrates this case.
Here we pass in the distance matrix along with the `ObsPerEvent` array.
The event-specific mean and covariance matrices are computed _on the fly_ within the `model` block.
This approach is far less efficient than the case where the spatial correlation is known in advance.
Note that for the particular example considered here, it is not a good idea to try to derive the correlation length in this manner. 
The example model being considered is too simplistic to adequately represent the scaling for all source-to-site combinations and so systematic biases will exist that become mapped into apparent spatial correlations.
Generally speaking, this is also true for more detailed analyses as the ergodic mixing of the dataset leads to biases from event-to-event that appear as apparent correlations and extend estimates of the correlation length.
Note also that studies like [@Jayaram:2009gm] also preferentially focus upon short inter-station separation distances when calibrating their correlation model as these distances are of greatest importance in practice and the consideration of larger separation distances tends to bias the computed correlation lengths.
That said, region-specific studies where separation distances among stations are not large [@Stafford:2018dn] have obtained consistent results using both approaches.

```{stan, output.var = "gmm_model_spatial_correlation_free"}
data {
  int<lower=1> N; // number of records
  int<lower=1> NEQ; // number of earthquakes
  int<lower=1> NSTAT; // number of earthquakes
  
  vector[N] M; // magnitudes
  vector[N] R; // distances
  vector[N] VS; // Vs30 values
  vector[N] Y; // ln PGA values
  
  int<lower=1,upper=NEQ> idx_eq[N];
  int<lower=1,upper=NSTAT> idx_stat[N];
  
  matrix[N,N] distanceMatrix; // inter-station distance matrix (for all records across all events)
  int<lower=1,upper=NEQ> ObsPerEvent[NEQ];
}

transformed data {
  real vref = 760;
}

parameters {
  real theta1;
  real theta2;
  real theta3;
  real theta4;
  real theta5;
  real theta6;
  real theta7;
  
  real<lower=0> h;
  
  real<lower=0> phiSS;
  real<lower=0> tau;
  real<lower=0> phiS2S;
  
  vector[NEQ] deltaB;
  vector[NSTAT] deltaS;

  real<lower=0> correlationLength;
}

model {
  vector[N] mu;

  theta1 ~ normal(0,10);
  theta2 ~ normal(0,10);
  theta3 ~ normal(0,10);
  theta4 ~ normal(0,10);
  theta5 ~ normal(0,10);
  theta6 ~ normal(0,10);
  theta7 ~ normal(0,10);
  h ~ normal(6,4);
  
  phiSS ~ cauchy(0,0.5);
  phiS2S ~ cauchy(0,0.5);
  tau ~ cauchy(0,0.5);
  
  deltaB ~ normal(0,tau);
  deltaS ~ normal(0,phiS2S);
  
  correlationLength ~ cauchy(5,1);

  for(i in 1:N) {
    mu[i] = theta1 + theta2 * M[i] + theta3 * square(M[i]) + (theta4 + theta5 * M[i]) * log(R[i] + h) + theta6 * R[i] + theta7 * log(VS[i]/vref) + deltaB[idx_eq[i]] + deltaS[idx_stat[i]];
  }

  {
    int epos;
    epos = 1;
    for ( i in 1:NEQ ) {
      if ( ObsPerEvent[i] > 1 ) {
        vector[ObsPerEvent[i]] eventMu = segment(mu, epos, ObsPerEvent[i]);
        matrix[ObsPerEvent[i],ObsPerEvent[i]] eventLCov = phiSS * cholesky_decompose( exp( -block(distanceMatrix, epos, epos, ObsPerEvent[i], ObsPerEvent[i] ) / correlationLength ) );
        segment(Y, epos, ObsPerEvent[i]) ~ multi_normal_cholesky( eventMu, eventLCov );
      } else {
        real eventMu = mu[epos];
        Y[epos] ~ normal( eventMu, phiSS );
      }
      epos = epos + ObsPerEvent[i];
    }
  }
}
```


```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
